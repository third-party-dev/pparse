# automatically generated by the FlatBuffers compiler, do not modify

# namespace: MNN

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class LinearAttentionParam(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = LinearAttentionParam()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsLinearAttentionParam(cls, buf, offset=0):
        """This method is deprecated. Please switch to GetRootAs."""
        return cls.GetRootAs(buf, offset)
    # LinearAttentionParam
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # LinearAttentionParam
    def AttnType(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            return self._tab.String(o + self._tab.Pos)
        return None

    # LinearAttentionParam
    def NumKHeads(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
        return 0

    # LinearAttentionParam
    def NumVHeads(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
        return 0

    # LinearAttentionParam
    def HeadKDim(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
        return 0

    # LinearAttentionParam
    def HeadVDim(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(12))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
        return 0

    # LinearAttentionParam
    def UseQkL2norm(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(14))
        if o != 0:
            return bool(self._tab.Get(flatbuffers.number_types.BoolFlags, o + self._tab.Pos))
        return False

def LinearAttentionParamStart(builder):
    builder.StartObject(6)

def Start(builder):
    LinearAttentionParamStart(builder)

def LinearAttentionParamAddAttnType(builder, attnType):
    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(attnType), 0)

def AddAttnType(builder, attnType):
    LinearAttentionParamAddAttnType(builder, attnType)

def LinearAttentionParamAddNumKHeads(builder, numKHeads):
    builder.PrependInt32Slot(1, numKHeads, 0)

def AddNumKHeads(builder, numKHeads):
    LinearAttentionParamAddNumKHeads(builder, numKHeads)

def LinearAttentionParamAddNumVHeads(builder, numVHeads):
    builder.PrependInt32Slot(2, numVHeads, 0)

def AddNumVHeads(builder, numVHeads):
    LinearAttentionParamAddNumVHeads(builder, numVHeads)

def LinearAttentionParamAddHeadKDim(builder, headKDim):
    builder.PrependInt32Slot(3, headKDim, 0)

def AddHeadKDim(builder, headKDim):
    LinearAttentionParamAddHeadKDim(builder, headKDim)

def LinearAttentionParamAddHeadVDim(builder, headVDim):
    builder.PrependInt32Slot(4, headVDim, 0)

def AddHeadVDim(builder, headVDim):
    LinearAttentionParamAddHeadVDim(builder, headVDim)

def LinearAttentionParamAddUseQkL2norm(builder, useQkL2norm):
    builder.PrependBoolSlot(5, useQkL2norm, 0)

def AddUseQkL2norm(builder, useQkL2norm):
    LinearAttentionParamAddUseQkL2norm(builder, useQkL2norm)

def LinearAttentionParamEnd(builder):
    return builder.EndObject()

def End(builder):
    return LinearAttentionParamEnd(builder)
